{
  "ok": true,
  "reason": "ok",
  "stdout": "[epoch=1] loss=5.768e+01 relL2=1.189e+00\n[epoch=200] loss=1.438e+00 relL2=1.115e-01\n[epoch=400] loss=3.138e-02 relL2=3.214e-03\n[epoch=500] loss=1.744e-02 relL2=1.971e-03\nTraining completed in 0.79 seconds. final relL2=1.971e-03\n{\"final_loss\": 0.017440810799598694, \"rel_l2\": 0.001970558660104871}\n",
  "stderr": "",
  "metrics": {
    "final_loss": 0.017440810799598694,
    "rel_l2": 0.001970558660104871,
    "loss_curve": [
      0.06618951261043549,
      0.067131407558918,
      0.07525516301393509,
      0.06336985528469086,
      0.05736130475997925,
      0.053913746029138565,
      0.0469120554625988,
      0.06174418330192566,
      0.06075418367981911,
      0.060427747666835785,
      0.051259689033031464,
      0.06113060191273689,
      0.06062707304954529,
      0.06485629826784134,
      0.04605594277381897,
      0.06430550664663315,
      0.04270920529961586,
      0.05569863319396973,
      0.06488075107336044,
      0.058401353657245636,
      0.05807068571448326,
      0.050148263573646545,
      0.0526428259909153,
      0.052933670580387115,
      0.06446177512407303,
      0.05013483390212059,
      0.07047240436077118,
      0.04642581567168236,
      0.05478508397936821,
      0.0575329065322876,
      0.055080559104681015,
      0.049412019550800323,
      0.04738904535770416,
      0.05533720552921295,
      0.05658813938498497,
      0.061211515218019485,
      0.045940104871988297,
      0.047333430498838425,
      0.05272559076547623,
      0.0556735135614872,
      0.038228701800107956,
      0.04385966435074806,
      0.050426024943590164,
      0.05487515777349472,
      0.04193848744034767,
      0.03792593255639076,
      0.04696468636393547,
      0.039664629846811295,
      0.04205156862735748,
      0.04256241396069527,
      0.03728117421269417,
      0.05665985494852066,
      0.04297802969813347,
      0.0391477532684803,
      0.04454539343714714,
      0.05531837418675423,
      0.05060938373208046,
      0.04337342455983162,
      0.0439627468585968,
      0.04156076908111572,
      0.05393475666642189,
      0.028242945671081543,
      0.046744681894779205,
      0.046336010098457336,
      0.04576506465673447,
      0.04504586383700371,
      0.04385178163647652,
      0.04767065867781639,
      0.03929818794131279,
      0.05341736599802971,
      0.039990369230508804,
      0.0495058111846447,
      0.052169833332300186,
      0.04167952015995979,
      0.050942450761795044,
      0.045369576662778854,
      0.05053674429655075,
      0.041454024612903595,
      0.03451540321111679,
      0.0396081767976284,
      0.031477294862270355,
      0.03796954080462456,
      0.0370509959757328,
      0.042815063148736954,
      0.04804731532931328,
      0.04594172537326813,
      0.045034684240818024,
      0.04052458703517914,
      0.05117044597864151,
      0.033349938690662384,
      0.04022945836186409,
      0.04710578918457031,
      0.0273330956697464,
      0.03960194066166878,
      0.030087662860751152,
      0.037620000541210175,
      0.031955182552337646,
      0.03303314745426178,
      0.03687722608447075,
      0.03138453885912895,
      0.03877812996506691,
      0.030253568664193153,
      0.027319109067320824,
      0.03460495546460152,
      0.02761388011276722,
      0.040016938000917435,
      0.0382634662091732,
      0.03825942054390907,
      0.031294405460357666,
      0.03255157545208931,
      0.03836164250969887,
      0.03428032249212265,
      0.02869403176009655,
      0.029526684433221817,
      0.03335331752896309,
      0.03522761911153793,
      0.03255770355463028,
      0.024747956544160843,
      0.028158752247691154,
      0.032457321882247925,
      0.03815125674009323,
      0.035964325070381165,
      0.0329386368393898,
      0.02490447647869587,
      0.024552127346396446,
      0.03309234604239464,
      0.04415624216198921,
      0.021547790616750717,
      0.02441955916583538,
      0.0363151989877224,
      0.03395060822367668,
      0.029485788196325302,
      0.025867514312267303,
      0.026460735127329826,
      0.02834438532590866,
      0.028093494474887848,
      0.027410272508859634,
      0.028355535119771957,
      0.03358526900410652,
      0.025501012802124023,
      0.030313260853290558,
      0.02152366004884243,
      0.02395343966782093,
      0.03184819966554642,
      0.028913039714097977,
      0.024663783609867096,
      0.025852039456367493,
      0.03215516358613968,
      0.024650897830724716,
      0.023560883477330208,
      0.025796515867114067,
      0.03224685788154602,
      0.0223774965852499,
      0.022594686597585678,
      0.028651705011725426,
      0.03158850222826004,
      0.02552971802651882,
      0.02263631485402584,
      0.023588547483086586,
      0.02175040729343891,
      0.03026474267244339,
      0.02768176980316639,
      0.02430410496890545,
      0.021908089518547058,
      0.02761130779981613,
      0.026970336213707924,
      0.030776377767324448,
      0.019847726449370384,
      0.03268199786543846,
      0.022499913349747658,
      0.02149510756134987,
      0.02294388972222805,
      0.030087541788816452,
      0.018496334552764893,
      0.028426233679056168,
      0.021884458139538765,
      0.028639892116189003,
      0.027746066451072693,
      0.02795873023569584,
      0.022222822532057762,
      0.024439822882413864,
      0.023645412176847458,
      0.017988210543990135,
      0.025334270671010017,
      0.02152775414288044,
      0.0305770356208086,
      0.020542491227388382,
      0.02423461340367794,
      0.017844928428530693,
      0.02151055447757244,
      0.021310903131961823,
      0.02256876602768898,
      0.021769195795059204,
      0.01793120987713337,
      0.020864803344011307,
      0.0224110446870327,
      0.02209935151040554,
      0.01954447105526924,
      0.016377681866288185,
      0.017440810799598694
    ]
  },
  "fragments": {
    "<<<MODEL_DEF>>>": "class PINN(nn.Module):\n    def __init__(self, hidden_layers=3, hidden_units=50):\n        super().__init__()\n        layers = []\n        in_dim = 1\n        for _ in range(hidden_layers):\n            layers.append(nn.Linear(in_dim, hidden_units))\n            layers.append(nn.Tanh())\n            in_dim = hidden_units\n        layers.append(nn.Linear(in_dim, 1))\n        self.net = nn.Sequential(*layers)\n\n        for m in self.net:\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_normal_(m.weight, gain=nn.init.calculate_gain('tanh'))\n                nn.init.zeros_(m.bias)\n\n    def forward(self, x):\n        return self.net(x)\n\n# <<<FORWARD_DEF>>>\ndef pinn_forward(model, x):\n    # x: (N,1) tensor of coordinates\n    x = x.requires_grad_(True)\n    u = model(x)\n    # first derivative\n    u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n    # second derivative\n    u_xx = torch.autograd.grad(u_x, x, grad_outputs=torch.ones_like(u_x), create_graph=True)[0]\n    return u, u_x, u_xx\n\n# alias expected name\nforward = pinn_forward\n\n# <<<HYPERPARAMS>>>\nHYPERPARAMS = {\n    \"epochs\": 5000,\n    \"lr\": 1e-3,\n    \"collocation\": 2000,\n    \"bc_weight\": 100.0,\n    \"verbose_every\": 100,\n    \"hidden_layers\": 3,\n    \"hidden_units\": 50,\n}",
    "<<<FORWARD_DEF>>>": "def pinn_forward(model, x):\n    # x: (N,1) tensor of coordinates\n    x = x.requires_grad_(True)\n    u = model(x)\n    # first derivative\n    u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n    # second derivative\n    u_xx = torch.autograd.grad(u_x, x, grad_outputs=torch.ones_like(u_x), create_graph=True)[0]\n    return u, u_x, u_xx\n\n# alias expected name\nforward = pinn_forward\n\n# <<<HYPERPARAMS>>>\nHYPERPARAMS = {\n    \"epochs\": 5000,\n    \"lr\": 1e-3,\n    \"collocation\": 2000,\n    \"bc_weight\": 100.0,\n    \"verbose_every\": 100,\n    \"hidden_layers\": 3,\n    \"hidden_units\": 50,\n}",
    "<<<HYPERPARAMS>>>": "HYPERPARAMS = {\n    \"epochs\": 5000,\n    \"lr\": 1e-3,\n    \"collocation\": 2000,\n    \"bc_weight\": 100.0,\n    \"verbose_every\": 100,\n    \"hidden_layers\": 3,\n    \"hidden_units\": 50,\n}"
  }
}