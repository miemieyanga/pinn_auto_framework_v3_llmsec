{
  "rel_l2": 0.001970558660104871,
  "config": {
    "<<<MODEL_DEF>>>": "class PINN(nn.Module):\n    def __init__(self, hidden_layers=3, hidden_units=50):\n        super().__init__()\n        layers = []\n        in_dim = 1\n        for _ in range(hidden_layers):\n            layers.append(nn.Linear(in_dim, hidden_units))\n            layers.append(nn.Tanh())\n            in_dim = hidden_units\n        layers.append(nn.Linear(in_dim, 1))\n        self.net = nn.Sequential(*layers)\n\n        for m in self.net:\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_normal_(m.weight, gain=nn.init.calculate_gain('tanh'))\n                nn.init.zeros_(m.bias)\n\n    def forward(self, x):\n        return self.net(x)\n\n# <<<FORWARD_DEF>>>\ndef pinn_forward(model, x):\n    # x: (N,1) tensor of coordinates\n    x = x.requires_grad_(True)\n    u = model(x)\n    # first derivative\n    u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n    # second derivative\n    u_xx = torch.autograd.grad(u_x, x, grad_outputs=torch.ones_like(u_x), create_graph=True)[0]\n    return u, u_x, u_xx\n\n# alias expected name\nforward = pinn_forward\n\n# <<<HYPERPARAMS>>>\nHYPERPARAMS = {\n    \"epochs\": 5000,\n    \"lr\": 1e-3,\n    \"collocation\": 2000,\n    \"bc_weight\": 100.0,\n    \"verbose_every\": 100,\n    \"hidden_layers\": 3,\n    \"hidden_units\": 50,\n}",
    "<<<FORWARD_DEF>>>": "def pinn_forward(model, x):\n    # x: (N,1) tensor of coordinates\n    x = x.requires_grad_(True)\n    u = model(x)\n    # first derivative\n    u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n    # second derivative\n    u_xx = torch.autograd.grad(u_x, x, grad_outputs=torch.ones_like(u_x), create_graph=True)[0]\n    return u, u_x, u_xx\n\n# alias expected name\nforward = pinn_forward\n\n# <<<HYPERPARAMS>>>\nHYPERPARAMS = {\n    \"epochs\": 5000,\n    \"lr\": 1e-3,\n    \"collocation\": 2000,\n    \"bc_weight\": 100.0,\n    \"verbose_every\": 100,\n    \"hidden_layers\": 3,\n    \"hidden_units\": 50,\n}",
    "<<<HYPERPARAMS>>>": "HYPERPARAMS = {\n    \"epochs\": 5000,\n    \"lr\": 1e-3,\n    \"collocation\": 2000,\n    \"bc_weight\": 100.0,\n    \"verbose_every\": 100,\n    \"hidden_layers\": 3,\n    \"hidden_units\": 50,\n}"
  }
}